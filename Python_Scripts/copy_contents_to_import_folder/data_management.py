# -*- coding: utf-8 -*-
"""
Created on Fri May 30 15:22:02 2014

@author: christoforos

This script is meant to pull the data from a server to a local directory.
Afterwhich the script analyzes the 01.data files and places them into a neat file.
"""
from os import walk, getcwd, system, remove, path, makedirs
from glob import glob
import numpy as np
import plot_lib
import sys
from subprocess import call #call waits for the command to compelte
from random import gauss


#####user data ########
#server = "clpashar@saw.sharcnet.ca"
server = "clpashar@saw.sharcnet.ca"
direc = "/work/clpashar/swap_mcs5000_brown"
run_2DNLCE = True #set to false if you don't want a bijillion directories
iterations_user = 1500
zombie_user = True #run the data with exact splicing?
order_type_user = {'Arithmetic':(2,7), 'Geometric':(4,36),'Quadratic':(2,100)}
#put hashtags in loop at the very bottom in front of analyze and rsync if you are not rsyncing and just want to use existing data that has its errors and entropies calculated

############


def rsync(login, directory, save_loc = ''):
    """Given the server name, username, will rsync data for -turaz \n
    t -> files in directory \n
    u -> update, i.e. skip newer files\n
    r -> transfers recursively \n
    a -> copies directories \n
    z -> compress \n
    
    only rsync's 01.data, to remove this functionality remove the include and exclude labels
    """
    
    if directory != "":
        if directory[-1] != '/':
            directory += '/'
        
        if directory[0]!= ":":
            directory = ":" + directory
    
    print "Running command:"
    rsync = "rsync -turazv --include '*/' %s%s %s." %(login, directory, save_loc)
    print rsync
    system(rsync)
    
#-------------------------just to copy 01.data
    #print "rsync -turazv --include '*/' --include '01.data' --exclude '*' %s%s %s." %(login, directory, save_loc)
    #system("rsync -turazv --include '*/' --include '01.data' --exclude '*' %s%s %s." %(login, directory, save_loc) )
#-------------------------
    
###############################################################################
def analyze(save_loc = '', prep_2DNLCE = True):
    """This script pulls in all the 01.data files in a directory
***SUBJECT TO CURRENT DIRECTORY ORGANIZATION***and then places them in a neat 
file with columns of <Ravi's Code> <Entropy_avg> <Error>
"""
        
    save_file = open('%sR_Entropy.data' %(save_loc), 'w')
    
    cwd = getcwd()  
    
    #makes it cleaner than recursive
    #will have to change if directory structure is changed
    progression_path = glob(cwd + '/*/*/Swap_Progression.dat')
    data01_path = glob(cwd + '/*/*/01.data')

#if statement for swap configuration-------------------------------------------
    if progression_path != []: #non empty
        #first see if Swap_Progression exists, if so do swap
        for loc in progression_path:
            
            root = loc.rstrip('/Swap_Progression.dat')
            
            #declare empty list types
            R_ent_swap = []
            err_swap = []
            tag_index = {}
            
            #first collect all the data from 01.data
            ID_data_path = glob(root + '/*/01.data')            
            counter = 0
            #num_bin_complete = 0 #keeps track of the smallest number of bins complete
            for cut_loc in ID_data_path:
                ID = cut_loc.split('/')[-2] #extract directory / tagID
                #handles loading in data
                data = np.loadtxt(cut_loc)
                
                #new_bins = len(data)
                #if bins > new_bins:
                    #bins = new_bins
                #if num_bin_complete  = len(data_in): #count the number of bins complete
                
                #make dictionary to keep track of where the data is the data variable
                tag_index.update({ID:counter})
                counter +=1 #update counter for data
                
                #make two lists of the swap entropy of one region (Ai+1/Ai) and its error
                R_ent_swap.append(-1*np.log(data.mean(axis=0))) #mean is consistant over all rebins
                #(below) propagation of error + plots of rebinning, if want one big
                #plot should amulgamate 01.data into ex 7x3 01.data with multiple columns
                #of the same amount of rows
                err_swap.append(1/data.mean(axis=0)*float(plot_lib.Re_Bin(path = root+'/'+ID, plot = False))) 

            #do the swap progression in Swap_Progression.dat generated by run_tfim_swap.py
            for swap_chain in open(loc, "r").readlines():
                if swap_chain == '':
                    #swap progression not generated yet
                    sys.exit("The Swap_Progressions.dat file has not been generated for %s yet,\
please wait a couple more minutes" %(root))
                
                #make the maths of swap
                R_ent_tot = 0
                var_tot = 0
                #loop over IDs in swap_chain
                #print swap_chain
                for tag in swap_chain.split():
                    #print tag, R_ent_swap[tag_index[tag]], err_swap[tag_index[tag]] 
                    #######ALGORITHM FOR SWAP BUILDING-------------------------
                    #now do propagation
                    R_ent_tot +=R_ent_swap[tag_index[tag]] #i.e. add the -log(S_{A_I})
                    var_tot += err_swap[tag_index[tag]]**2
                #print 'final result', swap_chain.split()[0], R_ent_tot, np.sqrt(var_tot)
                    #---------------------------------------------------------- 
                #write to R_Entropy and continue on to next progression
                save_file.write('%s\t %.10f\t %.10f\n' %(swap_chain.split()[0],
                                                R_ent_tot, np.sqrt(var_tot)))

#------------------------------------------------------------------------------            
#not sure if works      
    #check to see if 01.data isn't in progression path for mixture of swap and unswap, not operational
    for loc in data01_path:
        if loc not in progression_path:    
            
            print 'should not print'
            code = open(root + '/01.data', "r").readline()
            
            if code.startswith('#') == True: #if it starts with # then use ravi's codescheme
                code = code.strip("\n#")
                code = code.split()
            else: #only works well for one column
                code = root.split('/')[-1]                    
    
            data = np.loadtxt(root + "/01.data")
            R_ent = -1*np.log(np.mean(data,axis = 0)) #check with roger
            err = 1/data.mean(axis=0)*plot_lib.Re_Bin(path = root) #propagation of error
            
            if isinstance(R_ent, float):
                save_file.write("%s \t %.10f \t %.10f\n" %(code, R_ent, err))
            else:
                for entry in range(len(R_ent)):
                    try:
                        save_file.write("%s \t %.10f \t %.10f\n" %(code[entry], R_ent[entry], err[entry]))
                    except IndexError: #only one code, reuse code
                        code = code*len(R_ent)                     
         
    save_file.close()

###############################################################################

def Check_Orig(check_file_list = '', replace = False):
    """Will run all data retrieved against known analytic data for comparison
    check_file_name should have data arranged as tag in one column with the corner entropy in the next
    Also replaces the data with the original data for hybridized fitting if replace == True
    """
    #booby trap for checking []
    if check_file_list == '':
        check_file_name_list = ['../Ann_Check/Geometric_Ann', 
                           '../Ann_Check/Arithmetic_Miles'] #WARNING WILL NEED TO CHANGE IF YOU MOVE IT
                           
    for check_file_name in check_file_name_list:
        check_file = open(check_file_name, 'r')
        QMC_file_lines = open('R_Entropy.data', 'r').readlines()
        save_file = open('Diag_v_QMC','w')
        check_file_data = check_file.readlines()
        
        #replacement vars
        if replace == True:
            QMC_file = open('R_Entropy.data', 'w')
        overwrite = False
        #####
        
        for line in QMC_file_lines:
            cut = line.split()
            
            for line2 in check_file_data: #pick out tags if same tags write to the file
                cut2 = line2.split()
                #print cut2[0], cut[0]
                if cut2[0] == cut[0]:
                    save_file.write('\t'.join(cut2+cut[1:]) + '\n')# + '\t%.10f \t%.10f \t%.10f\n' %(cut[1], ent, err))
                    if replace == True: #copy to QMC file
                        QMC_file.write('\t'.join(cut2+['0.0']) + '\n') #add 0.0 for 0 error
                        overwrite = True

                    break #found tag, choose next tag in initial loop

            #write line if overwrite was not triggered
            if overwrite == False and replace == True:
                QMC_file.write('\t'.join(cut) + '\n')
            else:
                overwrite = False
            
        if replace == True:
            QMC_file.close()
        save_file.close()
        check_file.close()

###############################################################################

def Gaussian_Error_Prop(order_type = 'Arithmetic', order_range = (2,7),
                        iterations = 2000, cleanup = True, zombie = False):
    """This function takes the R_Entropy.data and passes it to mxn_nlce.py\n
    
    General stuff: generate the input files with the code for Ravi's code, not unique to order definition\n
    
    Step 1: Generate the unGaussied data and produce the Original_Results file\n
    
    Step 2: Generate the Gaussian data one at a time and produce the All_Results data file"""
        
    print 'Beginning Gaussian Error Propagation for : ' + order_type
    system('rm Results*') #cleanup before start
    tag,mu,std = np.loadtxt("R_Entropy.data", unpack = True, dtype= (str, 20)) #holds up to 20 digits, unpack data
    
    #remove files if they exist
    if path.isfile("All_Results"):#if file exists
        remove("All_Results")
    if path.isfile("Original_Results") == True:
        remove("Original_Results")       

###############################################################################
###########add necessary order_min, order_max, order_type lines in mxn_nlce.py#
#may change with editing script of mxn_nlce.py
    mxn = open('mxn_nlce.py', 'r')
    mxn_new = open('mxn_nlce_e.py', 'w')
    for line in mxn.readlines():
        if line.strip() == '#<\userdata>':# User settings
            mxn_new.write('order_min = %i\n' %(order_range[0]))
            mxn_new.write('order_max = %i\n' %(order_range[1]))
            if order_type == 'Arithmetic':
                mxn_new.write('order_step = 0.5\n')
            else:
                mxn_new.write('order_step = 1\n')
            mxn_new.write('order = %s()\n' %(order_type))
        else:
            mxn_new.write(line)
    mxn_new.close()
    mxn.close()
    
    #######First run the original data#########
    for j in range(len(tag)):
        #open the necessary file to write
        nlce_file = open(tag[j], 'w')
        nlce_file.write("2\t%.10f\n" %(float(mu[j]))) #heads up for rounding error
        nlce_file.close()
    system('chmod +x mxn_nlce_e.py')
    call('./mxn_nlce_e.py') #run nlce
    Merge_Results(save_file_name = "Original_Results") #move data to original_results
    system('rm Results*') #cleanup before run   
    #loop over iterations for gaussian noise
    for i in range(iterations):
        
        #now loop over all existing tags in R_Entropy.data for
        for j in range(len(tag)):
            nlce_file = open(tag[j], 'w')
            nlce_file.write("2\t%.10f\n" %(gauss(float(mu[j]), float(std[j])))) #heads up for rounding error
            nlce_file.close()
            
        call('./mxn_nlce_e.py') #run mxn        
        Merge_Results(save_file_name = "All_Results")  #move all data to big file
         
    if cleanup == True: #cleanup true removes all of the data used to use ./mxn_nlce.py
        for j in tag:
            remove(j)
        system('rm Results*')
    
    if zombie == True: 
        order_type +='_Zombie'
    system('rm -r %s' %(order_type))
    system('mkdir %s' %(order_type))
    system('cp All_Results %s/.' %(order_type))
    system('cp Original_Results %s/.' %(order_type))
    
    print 'Gaussian Propagation done'
    
    plot_lib.plot_universality(path = getcwd() + '/', order_type = order_type,
                               hybrid = zombie )

            
def Merge_Results(save_file_name = ''):
    """Takes the datafiles generated my mxn_nlce.py and adds them to the specified file
    """
    
    #save results and repeat        
    file_list = glob("Results_*")
    
    #open ze files appropriately
    if path.isfile(save_file_name) == True: #if file exists, don't print header
        header = False
        save_file = open(save_file_name, "a") #key a, needed for large amounts of data
    else: #otherwise open a new file
        header = True
        save_file = open(save_file_name, "w")
    #####
    
    #start getting the data to append
    temp = []        
    for file_name in file_list:
        open_file = open(file_name,"r")
        #now append by cutting off spaces and splitting str
        temp.append(open_file.readline().strip().split()[-1]) #only extract the value, not alpha
        open_file.close()
    holder = np.array(temp).T #transpose so that order names are at the top
    
    if header == True : #write header
        order = []
        for k in file_list:
            order.append(k.split('_')[-1])
            
        save_file.write('#'+ '\t'.join(order) + '\n')
    
    #write the data
    save_file.write('\t'.join(holder) + '\n')    
    save_file.close()   
        
if __name__ == '__main__' :
    
    rsync(server, direc)
    analyze(prep_2DNLCE = run_2DNLCE)
    Check_Orig(replace = zombie_user)
    
    if run_2DNLCE == True:
        for index in order_type_user:
            #run ravi's code
            Gaussian_Error_Prop(order_type = index, order_range = order_type_user[index],
                                iterations = iterations_user, cleanup = True, zombie = zombie_user)
        
            
        
